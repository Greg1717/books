---
title: "R Notebook"
output: html_notebook
editor_options: 
  chunk_output_type: console
---

# Purpose
This Notebook serves as a playground to practice topics learned:

- Applied Predictive Modeling
- Introduction to Statistical Learning

# Simple Linear Regression
## Data
```{r}
str(mtcars)
rownames(mtcars)
head(mtcars)
```


## Fit Model

We want to analyze how miles per gallon depend on displacement.
```{r}
# fit a linear model
lm_simple <- lm(mtcars$mpg ~ mtcars$disp)
# summary of linear model
summary(lm_simple)
```

## Plot Model
```{r}
plot(mtcars$disp, mtcars$mpg)
abline(lm_simple)
```

## Diagnostic Plots
```{r}
par(mfrow=c(2, 2))
plot(lm_simple)
par(mfrow = c(1, 1))
```


***
# Multiple Linear Regression
Review data
```{r}
summary(mtcars)
```

## Fit Model
We want to analyze how miles per gallon depend on displacement and weight.
```{r}
lm_mult <- lm(mtcars$mpg ~ ., data = mtcars)
summary(lm_mult)
```

## Diagnostic Plots
```{r}
par(mfrow=c(2, 2))
plot(lm_mult)
par(mfrow = c(1, 1))
```


***
# Logistic Regression

## Fit Model

Fit logistic regression model
vs	Engine (0 = V-shaped, 1 = straight)

```{r}
model <- stats::glm(vs ~ hp, data = mtcars, family = binomial)
summary(model)
```

```{r}
attributes(model)
```


## Interpretation

### Deviance

Whenever you fit a general linear model (like logistic regression, Poisson regression, etc.), most statistical software will produce values for the **null deviance** and **residual deviance** of the model.

The **null deviance** tells us how well the response variable can be predicted by a model with only an intercept term.

The **residual deviance** tells us how well the response variable can be predicted by a model with p predictor variables. The lower the value, the better the model is able to predict the value of the response variable.


### Chi-Square

To determine if a model is “useful” we can compute the **Chi-Square statistic**.

```{r}
chisq_log_reg <- mean(model$null.deviance) - mean(model$deviance)
chisq_log_reg
```


### P-value

To find the **p-value** that corresponds to a Chi-Square test statistic:
```{r}
stats::pchisq(q = chisq_log_reg, 
              df = model$df.null - model$df.residual,
              lower.tail = F)
```


## Predict

Define new data frame that contains predictor variable
```{r}
newdata <- data.frame(hp=seq(min(mtcars$hp), 
                             max(mtcars$hp),
                             len=500))
summary(newdata)
```

Use fitted model to predict values of vs
```{r}
newdata$vs = stats::predict(model, newdata, type = "response")
summary(newdata)
```

## Plot
Plot logistic regression curve
```{r}
plot(vs ~ hp, data = mtcars, col = "steelblue")
lines(vs ~ hp, data = newdata, lwd = 2)
```


***
# Rpart vs KNN vs Random Forest
```{r}
library(caret)
data(iris)
summary(iris)
```


## Plot Data

The following plot command from the caret package can show us how the Species values are distributed in a pairwise plot. An observation from this plot is that Versicolor and Virginica have similar patterns and Setosa is quite distinct. An intuition is that we can predict Setosa easily and might have some challenges in Versicolor and Virginica.

```{r}
#Visualising the data set
transparentTheme(trans = .4)

caret::featurePlot(
        x = iris[, 1:4],
        y = iris$Species,
        plot = "ellipse",
        auto.key = list(columns = 3)
)
```


## Partitioning the Data

Now we split up the data set into a train and a test partition. We use 80% of the data set to use into train and the remaining 20% into test. We also define a 10 fold cross validation method to be repeated 5 times. This process decreases over-fitting in the training set and helps the model work on an unknown or new data set. The model will be tested and trained several times on subsets of the training data to increase the accuracy in the test data.

```{r}
#Split into train and test data set
trainIndex <- createDataPartition(y = iris$Species,
                                  p = .8,
                                  list = FALSE,
                                  times = 1)
train <- iris[trainIndex, ]
test  <- iris[-trainIndex, ]
```


## Fit Model

Our problem is a Classification problem since our output predictor variable is a class. We have several machine learning algorithms for class prediction. We implement 3 methods here and compare them to find the best fit.


### Rpart

First fit a decision tree model (**Rpart**) to the train data set. A decision tree is a model with an “if this then that approach” and it is easy & fast to interpret with visualizing the tree.

We also preprocess the data set so that any variable with very different range of values will not affect our outcome.

```{r}
#Cross validation; Control the computational nuances of the train function
fitControl <- caret::trainControl(method = "repeatedcv",
                           number = 10,
                           repeats = 5)
fitControl
```

caret::train
```{r}
# caret::train - Fit Predictive Models over Different Tuning Parameters
# Rpart is a powerful machine learning library in R that is used for building classification and regression trees. This library implements recursive partitioning.
dt.fit <- caret::train(
        Species ~ .,
        data = train,
        method = "rpart",
        trControl = fitControl,
        preProcess = c("center", "scale")
)

dt.fit
```


**Accuracy** is the percentage of correctly classifies instances out of all instances. From the above results we can see that the model has a good accuracy value. **Kappa** is similar to accuracy but it is more based on a normalised random draw of the data set, i.e, it would be more useful for class imbalanced classifications. **CP** is the complexity parameter which is used to control the decision tree’s size and choose the optimal tree size. We can see that the model has chosen the tree size with the best accuracy.

Next, we predict the test data set using the trained model. A confusion matrix is used to understand the performance of the model. It is a table wise comparison of actual and predicted values. A variable importance plot shows that Petal width is the most important variable that has helped us predict the Species class.


#### Prediction

```{r}
predictions <- predict(dt.fit, test)
# the confusion matrix calculates a cross-tabulation of observed and predicted classes with associated statistics.
confusionMatrix(predictions, test$Species)
```

#### Variable Importance
```{r}
plot(caret::varImp(dt.fit))
```

The decision tree has predicted an accuracy of 93%


### KNN

The second algorithm we use is the K – Nearest neighbor algorithm. This is a great method for classification of the iris data set. In simple words, it takes inputs from the neighborhood data points and predicts the test data with confidence. K is the number of segments and the algorithm has chosen the best K value based on accuracy.

#### Fit
```{r}
knn.fit <- train(
        Species ~ .,
        data = train,
        method = "knn",
        trControl = fitControl,
        preProcess = c("center", "scale")
)

knn.fit
```

#### Prediction
```{r}
predictions <- predict(knn.fit, test)
confusionMatrix(predictions, test$Species)
```

#### varImp
```{r}
plot(varImp(knn.fit))
```
The KNN predicts with an accuracy of 93%


### Random Forest

The final method we use is the Random Forest method. This method uses a set of decision trees to aggregate the final results. This way we can minimize error caused from individual decision trees. The mtry value is the number of variables available for splitting of each tree node. Here again the optimal value model is selected based on accuracy.

#### Fit
```{r}
rf.fit <- train(
        Species ~ .,
        data = train,
        method = "rf",
        trControl = fitControl,
        preProcess = c("center", "scale")
)

rf.fit
```

#### Prediction
```{r}
predictions <- predict(rf.fit, test)
confusionMatrix(predictions, test$Species)
```

#### varImp
```{r}
plot(varImp(rf.fit))
```
The random forest predicts with an accuracy of 93%

From the above analysis we can see that all models have performed very well and therefore we can use the predictions from either of the model.


***
# PCA

## Base R

### Skewness

**Skewness** is a measure of the asymmetry of a distribution. A distribution is asymmetrical when its left and right side are not mirror images. A distribution can have right (or positive), left (or negative), or zero skewness.

A right-skewed distribution is longer on the right side of its peak, and a left-skewed distribution is longer on the left side of its peak.

Zero skew:  mean = median
Left skew: mean < median
Right skew: mean > median


#### Calc Skewness
```{r}
library(e1071)
skewness(mtcars$hp)
apply(mtcars, 2, skewness)
```

#### Transform Skewed Distributions
Find and apply appropriate transformation. 
```{r}
mtcars_trans <- caret::BoxCoxTrans(mtcars$hp)
mtcars_trans
# original data
head(mtcars$hp)
hist(mtcars$hp)
# transform
mtcars_hp_unskewed <- stats::predict(mtcars_trans, mtcars$hp)
hist(mtcars_hp_unskewed)
```

### stats::prcomp
```{r}
pcaObject <- stats::prcomp(mtcars,
                           center = TRUE,
                           scale. = TRUE)
pcaObject
pcaObject$sdev
# variable loadings:
pcaObject$rotation
pcaObject$center
pcaObject$scale
pcaObject$x
sum(pcaObject$sdev)
percent_variance <- pcaObject$sdev^2 / sum(pcaObject$sdev^2) * 100
percent_variance
plot(percent_variance)
lines(percent_variance)
perc_var_cumsum <- cumsum(percent_variance)
perc_var_cumsum
plot(perc_var_cumsum)
lines(perc_var_cumsum)
head(pcaObject$x)
```


## caret

```{r}
trans <- caret::preProcess(mtcars,
                           method = c("BoxCox", "center", "scale", "pca"))
trans
# variable loadings:
head(trans$rotation)
mtcars_transf_caret <- stats::predict(trans, mtcars)
head(mtcars_transf_caret)
```

## Filtering
### nearZeroVar
```{r}
library(caret)
caret::nearZeroVar(mtcars, saveMetrics = T)

nearZeroVar(iris[, -5], saveMetrics = TRUE)
data(BloodBrain)
nearZeroVar(bbbDescr)
nearZeroVar(bbbDescr, names = TRUE)
```

### Correlations
```{r}
correlations <- cor(mtcars)
correlations
library(corrplot)
corrplot(correlations)
corrplot(correlations, order = "hclust")
highCorr <- caret::findCorrelation(correlations, cutoff = 0.75)
highCorr
head(mtcars[, highCorr])
filtered_mtcars <- mtcars[, -highCorr]
filtered_mtcars
```

# Creating Dummy Variables
```{r}
library(caret)
str(mtcars)
mtcars_dummy <- mtcars
# convert gear to factors
mtcars_dummy$gear <- as.factor(mtcars_dummy$gear)
str(mtcars_dummy)
# dummyVars
mtcars_dummy_new <- caret::dummyVars(~gear,
                                 data = mtcars_dummy,
                                 levelsOnly = TRUE)
mtcars_dummy_new

stats::predict(mtcars_dummy_new, mtcars_dummy)
```


# PLS

source: https://www.statology.org/partial-least-squares-in-r/

One of the most common problems that you’ll encounter in machine learning is multicollinearity. This occurs when two or more predictor variables in a dataset are highly correlated.

When this occurs, a model may be able to fit a training dataset well but it may perform poorly on a new dataset it has never seen because it overfits the training set.

One way to get around this problem is to use a method known as partial least squares, which works as follows:

- Standardize both the predictor and response variables.
- Calculate M linear combinations (called “PLS components”) of the original p predictor variables that explain a significant amount of variation in both the response variable and the predictor variables.
- Use the method of least squares to fit a linear regression model using the PLS components as predictors.
- Use k-fold cross-validation to find the optimal number of PLS components to keep in the model.

For this example we’ll fit a partial least squares (PLS) model using hp as the response variable and the following variables as the predictor variables:

mpg
disp
drat
wt
qsec

```{r}
library(pls)
#make this example reproducible
set.seed(1)

#fit PCR model
model_pls <-
        plsr(
                hp ~ mpg + disp + drat + wt + qsec,
                data = mtcars,
                scale = TRUE,
                validation = "CV"
        )
summary(model_pls)
```



## Validation Plots

We can also visualize the test RMSE (along with the test MSE and R-squared) based on the number of PLS components by using the validationplot() function. 
```{r}
#visualize cross-validation plots
pls::validationplot(model_pls)
pls::validationplot(model_pls, val.type="MSEP")
pls::validationplot(model_pls, val.type="R2")
```



## Predictions
```{r}
#define training and testing sets
train <- mtcars[1:25, c("hp", "mpg", "disp", "drat", "wt", "qsec")]

y_test <- mtcars[26:nrow(mtcars), c("hp")]

test <- mtcars[26:nrow(mtcars), c("mpg", "disp", "drat", "wt", "qsec")]
    
#use model to make predictions on a test set
model_plsr <-
        plsr(
                hp ~ mpg + disp + drat + wt + qsec,
                data = train,
                scale = TRUE,
                validation = "CV"
        )

pcr_pred <- predict(model_plsr, test, ncomp = 2)

#calculate RMSE
sqrt(mean((pcr_pred - y_test)^2))
```


# Titanic
```{r load data Titanic}
train <- read.csv('kaggle_titanic/train.csv', stringsAsFactors = F)
test  <- read.csv('kaggle_titanic/test.csv', stringsAsFactors = F)
# full  <- rbind(train, test) # bind training & test data
# use the test file as our full titanic DB
titanic <- train
head(titanic)
```

```{r}
str(titanic)
```
Add family size:
```{r}
titanic$family_size <- 1 + titanic$SibSp + titanic$Parch
table(titanic$family_size)
```

Family sizes by Passanger Class:
```{r}
table(titanic$Pclass, titanic$family_size)
```


# Missing Data
```{r}
head(titanic[!complete.cases(titanic),], 17)
table(is.na(titanic$Age))
titanic <- titanic[!is.na(titanic$Age),]
hist(titanic$Age)
```
Add variable 'child' as logical.
```{r}
titanic$child <- ifelse(titanic$Age < 18, TRUE, FALSE)
hist(titanic$Age)
```

# Logistic Regression
```{r}
head(titanic)
log_regr_titanic <- glm(Survived ~ Pclass + Sex + Age,
                        data = titanic,
                        family = binomial)
summary(log_regr_titanic)
```






# Random Forest
```{r}
library(randomForest)
# Set a random seed
set.seed(754)

# Build the model (note: not all possible variables are used)
rf_model <- randomForest(factor(Survived) ~ Pclass + Sex + Age + SibSp + Parch + 
                                            Fare + family_size + child,
                                            data = titanic)

# Show model error
plot(rf_model, ylim=c(0,0.36))
legend('topright', colnames(rf_model$err.rate), col=1:3, fill=1:3)

```

```{r}

```

## Variable importance

Let's look at relative variable importance by plotting the mean decrease in Gini calculated across all trees.

```{r, message=FALSE, warning=FALSE}
# Get importance
importance    <- randomForest::importance(rf_model)
varImportance <- data.frame(Variables = row.names(importance), 
                            Importance = round(importance[ ,'MeanDecreaseGini'],2))
varImportance <- varImportance[order(-varImportance$Importance),]
# Create a rank variable based on importance
# rankImportance <- varImportance %>%
  # mutate(Rank = paste0('#',dense_rank(desc(Importance))))

library(ggplot2)
# Use ggplot2 to visualize the relative importance of variables
ggplot(rankImportance, aes(x = reorder(Variables, Importance), 
    y = Importance, fill = Importance)) +
  geom_bar(stat='identity') + 
  geom_text(aes(x = Variables, y = 0.5, label = Rank),
    hjust=0, vjust=0.55, size = 4, colour = 'red') +
  labs(x = 'Variables') +
  coord_flip() + 
  theme_few()
```


## Prediction!
We're ready for the final step --- making our prediction! When we finish here, we could iterate through the preceding steps making tweaks as we go or fit the data using different models or use different combinations of variables to achieve better predictions. But this is a good starting (and stopping) point for me now.

```{r}
# Predict using the test set
prediction <- predict(rf_model, test)

# Save the solution to a dataframe with two columns: PassengerId and Survived (prediction)
solution <- data.frame(PassengerID = test$PassengerId, Survived = prediction)

# Write the solution to file
write.csv(solution, file = 'rf_mod_Solution.csv', row.names = F)
```

