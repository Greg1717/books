---
title: "R Notebook"
output: html_notebook
editor_options: 
  chunk_output_type: inline
---


# Part I General Strategies

## 3 Data Pre-Processing

### Computing

There were fields that identified each cell (called Cell) and a factor vector that indicated which cells were well segmented (Class). The variable Case indicated which cells were originally used for the training and test sets.  The analysis in this chapter focused on the training set samples, so the data are filtered for these cells:

```{r}
apropos("matrix")
RSiteSearch("confusion", restrict = "functions")

library(AppliedPredictiveModeling)
library(data.table)

data(segmentationOriginal)
segmentationOriginal
str(segmentationOriginal)
sapply(segmentationOriginal, class)
segData <- as.data.table(segmentationOriginal)
# filter on training data
segData <- segData[Case == "Train"]
CellID <- segData$Cell
class <- segData$Class
case <- segData$Case
# Now remove the saved columns
segData <- segData[, -(1:3)]
# remove 'Status' columns
statusColNum <- grep("Status", names(segData))
statusColNum
segData <- segData[, -..statusColNum]
```


#### Transformations

```{r}
# As previously discussed, some features exhibited significantly skewness. The skewness function in the e1071 package calculates the sample skewness statistic for each predictor:

library(e1071)

# For one predictor:
skewness(segData$AngleCh1)

# Since all the predictors are numeric columns, the apply function can be used to compute the skewness across columns.

skewValues <- apply(segData, 2, skewness)
head(skewValues)
hist(segData$EqEllipseOblateVolCh1)

# Using these values as a guide, the variables can be prioritized for visualizing the distribution.  The basic R function hist or the histogram function in the lattice can be used to assess the shape of the distribution. To determine which type of transformation should be used, the MASS package contains the boxcox function. Although this function estimates λ, it does not create the transformed variable(s).

# A caret function, BoxCoxTrans, can find the appropriate transformation and apply them to the new data:

library(caret)
Ch1AreaTrans <- caret::BoxCoxTrans(segData$AreaCh1)
Ch1AreaTrans
 
# The original data
head(segData$AreaCh1)

# After transformation
predict(Ch1AreaTrans, head(segData$AreaCh1))
(819^(-.9) - 1)/(-.9)
```

Another caret function, **preProcess**, applies this transformation to a set of predictors. This function is discussed below. 

The base R function **prcomp** can be used for PCA.  In the code below, the data are centered and scaled prior to **PCA**:

```{r}
pcaObject <- prcomp(segData, 
                    center = TRUE, 
                    scale. = TRUE)

# Calculate the cumulative percentage of variance which each component accounts for.

percentVariance <- pcaObject$sd^2/sum(pcaObject$sd^2)*100
percentVariance[1:3]

# The transformed values are stored in pcaObject as a sub-object called x:

head(pcaObject$x[, 1:5])
```

The another sub-object called **rotation** stores the **variable loadings**, where rows correspond to predictor variables and columns are  associated with the components:

```{r}
head(pcaObject$rotation[, 1:3])
```

The caret package class spatialSign contains functionality for the **spatial sign transformation**. Although we will not apply this technique to these data, the basic syntax would be spatialSign(segData).  

Also, these data do not have **missing values** for imputation. To impute missing values, the impute package has a function, **impute.knn**, that uses K-nearest neighbors to estimate the missing data. The previously mentioned preProcess function applies imputation methods based on K-nearest neighbors or bagged trees.  

To administer a series of transformations to multiple data sets, the **caret class preProcess** has the ability to 

 - transform, 
 - center, 
 - scale, or 
 - impute values, as well as 
 - apply the spatial sign transformation and 
 - feature extraction. 

The function calculates the required quantities for the transformation. After calling the preProcess function, the predict method applies the results to a set of data. For example, to Box–Cox transform, center, and scale the data, then execute PCA for signal extraction, the syntax would be:

```{r}
trans <- 
        caret::preProcess(segData, 
                          method = c("BoxCox", "center", "scale", "pca"))
trans
trans$dim
trans$rotation

# Apply the transformations:
transformed <- predict(trans, segData)

# These values are different than the previous PCA components since they were transformed prior to PCA
head(transformed[, 1:5])
```

The **order** in which the possible transformation are applied is: 
 
 - transformation, 
 - centering, 
 - scaling, 
 - imputation, 
 - feature extraction, and then 
 - spatial sign.  
 
Many of the modeling functions have options to center and scale prior to modeling.  For example, when using the train function (discussed in later chapters), there is an option to use preProcess prior to modeling within the resampling iterations.


#### Filtering

To filter for **near-zero variance predictors**, the caret package function **nearZeroVar** will return the column numbers of any predictors that fulfill the conditions outlined in Sect. 3.5. For the cell segmentation data, there are no problematic predictors:

```{r}
nearZeroVar(segData)

# When predictors should be removed, a vector of integers is returned that indicates which columns should be removed.

# Similarly, to filter on between-predictor correlations, the cor function can calculate the correlations between predictor variables:

correlations <- cor(segData)
dim(correlations)
correlations[1:4, 1:4]
str(correlations)
```


##### corrplot()

To visually examine the correlation structure of the data, the corrplot pack-
age contains an excellent function of the same name. The function has many
options including one that will reorder the variables in a way that reveals
clusters of highly correlated predictors.

```{r}
library(corrplot)
corrplot(correlations, order = "hclust")
```

The size and color of the points are associated with the strength of correlation
between two predictor variables.
To filter based on correlations, the **findCorrelation** function will apply the
algorithm in Sect. 3.5. For a given threshold of pairwise correlations, the func-
tion returns column numbers denoting the predictors that are recommended
for deletion:

```{r}
highCorr <- findCorrelation(correlations, cutoff = .75)
length(highCorr)
head(highCorr)
filteredSegData <- segData[, -highCorr]
```


#### Creating Dummy Variables

Several methods exist for creating dummy variables based on a particular model. Section 4.9 discusses different methods for specifying how the predictors enter into the model. One approach, the formula method, allows great flexibility to create the model function. Using formulas in model functions parameterizes the predictors such that not all categories have dummy variables. This approach will be shown in greater detail for linear regression. As previously mentioned, there are occasions when a complete set of dummy variables is useful. For example, the splits in a tree-based model are more interpretable when the dummy variables encode all the information for that predictor. We recommend using the full set if dummy variables when working with tree-based models.  

Example dummyVars:

```{r}

when <- data.frame(time = c("afternoon", "night", "afternoon",
                            "morning", "morning", "morning",
                            "morning", "afternoon", "afternoon"),
                   day = c("Mon", "Mon", "Mon",
                           "Wed", "Wed", "Fri",
                           "Sat", "Sat", "Fri"),
                           stringsAsFactors = TRUE)

when

levels(when$time) <- list(morning="morning",
                          afternoon="afternoon",
                          night="night")
when

levels(when$day) <- list(Mon="Mon", Tue="Tue", Wed="Wed", Thu="Thu",
                         Fri="Fri", Sat="Sat", Sun="Sun")

## Default behavior:
model.matrix(~day, when)

mainEffects <- dummyVars(~ day + time, data = when)
mainEffects
predict(mainEffects, when[1:3,])

when2 <- when
when2[1, 1] <- NA
when2
when2[1:3,]
predict(mainEffects, when2[1:3,])
predict(mainEffects, when2[1:3,], na.action = na.omit)


interactionModel <- dummyVars(~ day + time + day:time,
                              data = when,
                              sep = ".")

predict(interactionModel, when[1:3,])

noNames <- dummyVars(~ day + time + day:time,
                     data = when,
                     levelsOnly = TRUE)
predict(noNames, when)

head(class2ind(iris$Species))

two_levels <- factor(rep(letters[1:2], each = 5))
class2ind(two_levels)
class2ind(two_levels, drop2nd = TRUE)
```


### Exercises

#### 3.1. 

The UC Irvine Machine Learning Repository contains a data set related to glass identification. The data consist of 214 glass samples labeled as one of seven class categories. There are nine predictors, including the refractive index and percentages of eight elements: Na, Mg, Al, Si, K, Ca, Ba, and Fe. The data can be accessed via:

(a) Using visualizations, **explore** the predictor variables to understand their
distributions as well as the relationships between predictors.  
(b) Do there appear to be any **outliers** in the data? Are any predictors **skewed**?  
(c) Are there any relevant transformations of one or more predictors that might improve the classification model?  


```{r}
library(mlbench)
data(Glass)
str(Glass)
head(Glass)
```


##### Explore

```{r}
library(reshape2)
meltedGlass <- melt(Glass, id.vars = "Type")
head(meltedGlass)

# Now, we can use the lattice function densityplot to look at each predictor:
library(lattice)

lattice::densityplot(~value|variable,
            data = meltedGlass,
            ## Adjust each axis so that the measurement scale is
            ## different for each panel
            scales = list(x = list(relation = "free"),
                          y = list(relation = "free")),
            ## 'adjust' smooths the curve out
            adjust = 1.25,
            ## change the symbol on the rug for each data point
            pch = "|",
            xlab = "Predictor")
```
We can see that K and Mg appear to have possible **second modes around zero** and that several predictors (Ca, Ba, Fe and RI) show signs of **skewness**. There may be one or two **outliers** in K, but they could simply be due to natueral skewness. Also, predictors Ca, RI, Na and Si have concentrations of samples in the middle of the scale and a small number of data points at the edges of the distribution. This characteristic is indicative of a **heavy–tailed distribution**.

A scatterplot matrix can also be helpful to visualize a data set of this size. 

```{r}
?pairs
pairs(Glass)
```

```{r}
library(GGally)
?ggpairs
ggpairs(Glass)
```

This visualization highlights several other important characteristics of this data:

1. The **measurements** of some glass types, specifically Fe, Ba, K and Mg, are **zero**. This creates a “mixture distribution” of points; one distribution consists of glass types containing the element in question whereas the other does not. This finding implies that the samples in these distributions may not behave in the same manner.
2. Most predictors are uncorrelated with the exception of pairs Ca/RI and maybe RI/Si.
3. Many of the pair–wise combinations have very **non–standard distributions** (i.e. **heavy tails** or mixtures of distributions).
4. It is difficult to tell if the extreme point in the K data is an **outlier** or just a artifact of a skewed distribution that has not been sampled enough. In either case, this should be accounted for through the modeling, preferably by using models that are resistant to outliers.

Another nice visualization of correlations:
```{r}
library(corrplot)
Glass_cor <- cor(Glass[, 1:9])
?corrplot
corrplot(Glass_cor, order = "hclust")
corrplot(Glass_cor, method = c("number"), order = "hclust")
```


##### Skewness & Outliers

Would **transformations** help these data? 

Based on our findings above, we need to investigate transformations of individual predictors that will **resolve skewness and/or outliers** (e.g. the spatial sign transformation).

For skewness, first note that several predictors have **values of zero**. This excludes transformations such as the log transformation or the Box–Cox family of transformations. When we are faced with predictors containing zero values, the Yeo–Johnson family of transformations can be helpful (Yeo & Johnson 2000). This family of transformations is very similar to the Box–Cox transformation, but can handle zero or negative predictor values. The transformation can be estimated using caret’s preProcess function:

```{r}
library(caret)
yjTrans <- caret::preProcess(Glass[, -10], method = "YeoJohnson")
yjTrans
yjData <- predict(yjTrans, newdata = Glass[, -10])
yjData
melted <- melt(yjData)
melted
```

The resulting density plots are shown in Figure 3. The only substantive change relative to the original distributions is that a second mode was induced for predictors Ba and Fe. Given these results, **this transformation did not seem to improve the data (in terms of skewness)**. 

Next, we will apply the **spatial sign** transformation to attempt to **mitigate outliers**. For this data, we first center and scale the data, then apply the transformation:

```{r}
centerScale <- caret::preProcess(Glass[, -10], method = c("center", "scale"))
centerScale
csData <- predict(centerScale, newdata = Glass[, -10])
csData
ssData <- caret::spatialSign(csData)
ssData

lattice::splom(~ssData, pch = 16, col = rgb(.2, .2, .2, .4), cex = .7)
```


Many of the possible outliers have been contracted into the mainstream of the data. This transformation did result in at least one new pattern: the samples with **zero values** for both Fe and B are now projected onto a straight line in these two dimensions. While we were unable to resolve skewness in this data via transformations, we were able to minimize the number of unusually extreme observations. 

Note that attempts to pre–process data to resolve predictor distribution problems are not always successful. Our best efforts in pre–processing may not yield highly desirable transformed values. Under these kinds of circumstances, we will need to use models that are not unduly affected by skewed distributions (e.g. tree–based methods).


```{r}

```

```{r}

```





#### 3.2. Frequency distributions & missing values

The soybean data can also be found at the UC Irvine Machine Learning
Repository. Data were collected to predict disease in 683 soybeans. The 35 predictors are mostly categorical and include information on the environmental conditions (e.g., temperature, precipitation) and plant conditions (e.g., left spots, mold growth). The outcome labels consist of 19 distinct classes.

The data can be loaded via:
```{r}
library(mlbench)
data(Soybean)
?Soybean
Soybean
table(Soybean$Class)
str(Soybean)
```

See ?Soybean for details

(a) Investigate the **frequency distributions for the categorical predictors**. Are any of the distributions degenerate in the ways discussed earlier in this chapter?

(b) Roughly 18 % of the data are **missing**. Are there particular predictors that are more likely to be missing? Is the pattern of missing data related to the classes?

(c) Develop a **strategy for handling missing data**, either by eliminating predictors or imputation.


##### table()

```{r}
Soybean2 <- Soybean
table(Soybean2$temp, useNA = "always")
```

```{r}
# alternative method
library(car)
Soybean2$temp <- car::recode(Soybean2$temp,
                             "0 = 'low'; 1 = 'norm'; 2 = 'high'; NA = 'missing'",
                             levels = c("low", "norm", "high", "missing"))

# NA is not changed to 'missing' with this (base R) method
# levels(Soybean2$temp) <- c("low", "norm", "high")

table(Soybean2$temp)
```
For this part of the solution to this problem, we will look at the relationship between the months, temperature and precipitation. To explore these relationships, we need to recode months and precipitation. 

```{r}
table(Soybean2$date, useNA = "always")
Soybean2$date <- 
  car::recode(Soybean2$date,
              "0 ='apr';1='may';2='june';3='july';4='aug';5='sept';6='oct';NA = 'missing'",
              levels = c("apr", "may", "june", "july", "aug", "sept", "missing"))
table(Soybean2$date)
```

```{r}
table(Soybean2$precip, useNA = "always")

Soybean2$precip <- car::recode(Soybean2$precip, 
                               "0 = 'low'; 1 = 'norm'; 2 = 'high'; NA = 'missing'",
                               levels = c("low", "norm", "high", "missing"))
table(Soybean2$precip)
```
To start, let’s look at the date predictor. Are the months represented equally? From the table
above, we can see that June through September have the most data and that there is a single
missing value. For precipitation (ironically) most of the data are above average. In addition, the
temperature and precipitation columns have missing value rates of about 5%.


##### vcd::mosaic()

Like the previous problems, we should examine the pair-wise or joint distributions of these predic-
tors. Joint distributions of factor predictors are often displayed in a contingency table. There are
also several ways that these distributions can be displayed in a graph. The mosaic function in the
vcd package (Meyer, Zeileis & Hornik 2006) and the barchart function in the lattice package are
two options. What does the joint distribution of temperature and month look like? First, we will
use a mosaic plot:

```{r}
library(vcd)
## mosaic() can table a table or a formula:
vcd::mosaic(~date + temp, data = Soybean2)
```

```{r}
table(Soybean2$date, Soybean2$temp)
```

##### lattice::barchart

Alternatively, a bar chart can also be used:

```{r}
lattice::barchart(table(Soybean2$date, Soybean2$temp),
                  auto.key = list(columns = 4, title = "temperature"))
```
The results are shown in Figure 5. Note that in the bar chart, the bars are not cumulative (i.e.
missing values are not the most frequent). Here we see which months are the most frequent.
Additionally, we see that average temperatures are the most frequent category within each month,
although high temperatures are also very likely in September. Missing values are most likely in
July. One useful option to barchart is stack to create stacked bars.
To investigate higher–order relationships, predictors can be added to the table or formula to create
more complex visualizations (e.g. panels in the lattice plots, etc).
What does the distribution look like per response class for the missing data? If we look at the
frequency of any missing predictor value per class, the results show that some classes are more
problematic than others:

```{r}
table(Soybean$Class, complete.cases(Soybean))
```
```{r}
hasMissing <- unlist(lapply(Soybean, function(x) any(is.na(x))))
hasMissing
hasMissing <- names(hasMissing)[hasMissing]
head(hasMissing)
```
There are several classes where all of the samples have at least one missing predictor value. Are
these concentrated in a single predictor that we could remove? We can get the percentage of missing
values for each predictor by class using the following syntax:

```{r}
byPredByClass <- apply(Soybean[, hasMissing], 
                       2,
                       function(x, y) {
                         tab <- table(is.na(x), y)
                         tab[2, ] / apply(tab, 2, sum)
                       },
                       y = Soybean$Class)

## The columns are predictors and the rows are classes. Let's eliminate
## any rows and columns with no missing values

byPredByClass <- byPredByClass[apply(byPredByClass, 1, sum) > 0, ]
byPredByClass <- byPredByClass[, apply(byPredByClass, 2, sum) > 0]

## now print:
t(byPredByClass)
```
From this output, we see that there are many predictors completely missing for the 2-4-d-injury,
cyst-nematode and herbicide-injury classes. The phytophthora-rot class has a high rate of
missing data across many predictors and the diaporthe-pod-&-stem-blight has a more moderate
pattern of missing data.

One approach to handling missing data is to use an imputation technique. However, it is unlikely
that imputation will help since almost 100% of the predictor values will need to be imputed in a
few cases. We could encode the missing as another level or eliminate the classes associated with
the high rate of missing values from the data altogether.

##### caret::dummyVars

How would the frequencies of the predictor values affect the modeling process? If we are using a
model that is sensitive to sparsity then the low rate of some of the factor levels might be an issue.
We can convert the factors to a set of **dummy variables** and see how good or bad the sparsity is.
```{r}
## Some of the factors are ordinal. First convert them to unordered factors so
## that we get a set of binary indicators.

orderedVars <- unlist(lapply(Soybean, is.ordered))
orderedVars <- names(orderedVars)[orderedVars]
orderedVars

## Let's bypass the problem of missing data by removing the offending classes

completeClasses <- as.character(unique(Soybean$Class[complete.cases(Soybean)]))

Soybean3 <- subset(Soybean, Class %in% completeClasses)

for(i in orderedVars) Soybean3[, i] <- factor(as.character(Soybean3[, i]))

## Use dummyVars to generate the binary predictors...
library(caret)
dummyInfo <- caret::dummyVars(Class ~ ., data = Soybean3)

dummyInfo$vars

dummies <- predict(dummyInfo, Soybean3)
head(dummies[1:4,1:14])
```
... then nearZeroVar to figure out which should be removed.
```{r}
predDistInfo <- caret::nearZeroVar(dummies, saveMetrics = TRUE)
head(predDistInfo)
```


The number and percentage of predictors to remove:
```{r}
sum(predDistInfo$nzv)
mean(predDistInfo$nzv)
```
So if we wanted to remove sparse and unbalanced predictors, 16.2% of the dummy variables would
be eliminated. One way around this is to use models that are not sensitive to this characteristic,
such as tree– or rule–based models, or na¨ıve Bayes.





#### 3.3. 

Chapter 5 introduces Quantitative Structure-Activity Relationship (QSAR) modeling where the characteristics of a chemical compound are used to predict other chemical properties. The caret package contains a QSAR data set from Mente and Lombardo (2005). Here, the ability of a chemical to permeate the blood-brain barrier was experimentally determined for 208 compounds. 134 descriptors were measured for each compound. 

(a) Start R and use these commands to load the data:
```{r}
library(caret)
data(BloodBrain)
?BloodBrain
head(bbbDescr)
```

The numeric outcome is contained in the vector logBBB while the predictors are in the data frame bbbDescr. 

(b) Do any of the individual predictors have **degenerate distributions**?
(c) Generally speaking, are there strong relationships between the predictor data? If so, how could **correlations** in the predictor set be reduced? Does this have a dramatic effect on the number of predictors available for modeling?

Solutions
For these data, the first assessment looks for **sparse and unbalanced predictors**. The caret nearZeroVar function is used again but this time with the saveMetrics options that retains information about each predictor:

```{r}
predictorInfo <- nearZeroVar(bbbDescr, saveMetrics = TRUE)
head(predictorInfo)
```
Are there any near-zero variance predictors?
```{r}
rownames(predictorInfo)[predictorInfo$nzv]
```
Examples:
```{r}
table(bbbDescr$a_acid)
table(bbbDescr$alert)
## Let's get rid of these:
filter1 <- bbbDescr[, !predictorInfo$nzv]
ncol(filter1)

```
As mentioned in the text, there are some models that are resistant to near–zero variance predictors and, for these models, we would most likely leave them in. 

What about the distributions of the remaining predictors? Although, it is time consuming to look at individual density plots of 127 predictors, we do recommend it (or at least looking at a sample of predictors). For example, the top panel of Figure 6 shows a random sample of eight predictors:

```{r}
set.seed(532)
sampled1 <- filter1[, sample(1:ncol(filter1), 8)]
names(sampled1)
```
A few of these predictors exhibit skewness and one (frac.cation7.) shows two distinct modes.
Based on the rug plot of points in the panel for o sp2, these data are also likely to be bimodal.
To numerically assess skewness, the function from the e1071 package is used again:

```{r}
library(e1071)
skew <- apply(filter1, 2, e1071::skewness)
summary(skew)
```
There are a number of predictors that are left– or right–skewed. We can again apply the Yeo–
Johnson transformation to the data (some of the predictors are negative):
```{r}
yjBBB <- preProcess(filter1, method = "YeoJohnson")

transformed <- predict(yjBBB, newdata = filter1)

sampled2 <- transformed[, names(sampled1)]

sampled2
```
The results for the sampled predictors are shown in the bottom panel of Figure 6. Although the distributions for fpsa3 and wpsa2 are more symmetric, the other predictors have either **additional modes** or more pronounced modes. One option would be to manually assess which predictors would benefit from this type of transformation.

Is there severe **correlation** between the predictors? Based on previous experience with these types of data, there are likely to be many relationships between predictors. For example, when we examine the predictor names we find that 24 are some type of surface area predictor. These are most likely **correlated** to some extent. Also, surface area is usually related to the size (or weight) of a molecule, so additional correlations may exist. 

The **correlation matrix** of the predictors can be computed and examined. However, we know that many predictors are skewed in these data. Since the correlation is a function of squared values of the predictors, the samples in the tails of the predictor distributions may have a significant effect on
the correlation structure. For this reason, we will look at the correlation structure three ways: 

- the untransformed data, 
- the data after the Yeo–Johnson transformation, and 
- the data after a spatial sign transformation.

```{r}
rawCorr <- cor(filter1)
rawCorr
transCorr <- cor(transformed)
transCorr
ssData <- spatialSign(scale(filter1))
ssCorr <- cor(ssData)
library(corrplot)
## plot the matrix with no labels or grid
corrplot(rawCorr, order = "hclust", addgrid.col = NA, tl.pos = "n")
```
```{r}
corrplot(transCorr, order = "hclust", addgrid.col = NA, tl.pos = "n")
```

```{r}
ssData <- spatialSign(scale(filter1))
ssCorr <- cor(ssData)
corrplot(ssCorr, order = "hclust", addgrid.col = NA, tl.pos = "n")
```
This visualization indicates that correlations lessen with increasing levels of transformations:
```{r}
corrInfo <- function(x) summary(x[upper.tri(x)])
corrInfo(rawCorr)
```

```{r}
corrInfo(transCorr)
```

```{r}
corrInfo(ssCorr)
```
Rather than transform the data to resolve between–predictor correlations, it may be a better idea to remove predictors. The caret function **findCorrelation** was described in the text. The user is required to state what level of pair–wise correlations that they are willing to accept. The code below shows (for these data) the trade–off between the correlation threshold, the number of retained predictors, and the average absolute correlation in the data. Figure 8 shows the results.
```{r}
thresholds <- seq(.25, .95, by = 0.05)
size <- meanCorr <- rep(NA, length(thresholds))
removals <- vector(mode = "list", length = length(thresholds))


for (i in seq_along(thresholds)) {
  removals[[i]] <- findCorrelation(rawCorr, thresholds[i])
  subMat <- rawCorr[-removals[[i]], -removals[[i]]]
  size[i] <- ncol(rawCorr) - length(removals[[i]])
  meanCorr[i] <- mean(abs(subMat[upper.tri(subMat)]))
}

corrData <- data.frame(
  value = c(size, meanCorr),
  threshold = c(thresholds, thresholds),
  what = rep(
    c("Predictors",
      "Average Absolute Correlation"),
    each = length(thresholds)
  )
)
corrData
```

```{r}
caret::findCorrelation(rawCorr, cutoff = 0.98, names = T)
```
We can also try the subselect package (Cerdeira, Silva, Cadima & Minhoto 2014) to remove predic-
tors. This package uses a different criterion to evaluate the quality of a subset and has less greedy
methods to search the predictor space. First, we have to remove all linear dependencies from the
data. That includes perfect pair–wise correlations as well as relationships between three or more
predictors. The trim.matrix function does that:
```{r}
library(subselect)
ncol(rawCorr)
trimmed <- trim.matrix(rawCorr, tolval=1000*.Machine$double.eps)$trimmedmat
ncol(trimmed)
```
We can use simulated annealing and genetic algorithms to search for quality subsets. These techniques allow for lower and upper limits for the number of predictors. However, the functions get dramatically slower as the range increases. Here, we will look at one solution found by findCorrelation and, will subsequently use subselect to search within that subset size:
```{r}
set.seed(702)

# Given a set of variables, a Simulated Annealing algorithm seeks a k-variable subset which is optimal, as a surrogate for the whole set, with respect to a given criterion.
sa <- subselect::anneal(trimmed, kmin = 18, kmax = 18, niter = 1000)

saMat <- rawCorr[sa$bestsets[1,], sa$bestsets[1,]]

set.seed(702)

# Given a set of variables, a Genetic Algorithm algorithm seeks a k-variable subset which is optimal, as a surrogate for the whole set, with respect to a given criterion.
ga <- subselect::genetic(trimmed, kmin = 18, kmax = 18, nger = 1000)

gaMat <- rawCorr[ga$bestsets[1,], ga$bestsets[1,]]

fcMat <- rawCorr[-removals[size == 18][[1]],
                 -removals[size == 18][[1]]]

corrInfo(fcMat)
```
```{r}
corrInfo(saMat)
```

```{r}
corrInfo(gaMat)
```

The main difference between these results is that the greedy approach of **findCorrelation** is much more conservative than the techniques found in the subselect package.



## 4 Over-Fitting and Model Tuning
### Computing
### Exercises

# Part II Regression Models
## 5 Measuring Performance in Regression Models
## 6 Linear Regression and Its Cousins
## 7 Nonlinear Regression Models
## 8 Regression Trees and Rule-Based Models
## 9 A Summary of Solubility Models
## 10 Case Study: Compressive Strength of Concrete Mixtures

# Part III Classification Models
## 11 Measuring Performance in Classification Models
## 12 Discriminant Analysis and Other Linear Classification Models
## 13 Nonlinear Classification Models
## 14 Classification Trees and Rule-Based Models
## 15 A Summary of Grant Application Models 
## 16 Remedies for Severe Class Imbalance
## 17 Case Study: Job Scheduling

# Part IV Other Considerations
## 18 Measuring Predictor Importance
## 19 An Introduction to Feature Selection
## 20 Factors That Can Affect Model Performance



